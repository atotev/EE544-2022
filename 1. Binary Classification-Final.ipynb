{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import path\n",
    "from os import environ\n",
    "import os\n",
    "import random as random\n",
    "from tensorflow.keras.preprocessing import image as imgproc\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to make runs more reproducible\n",
    "seed_value=20212042\n",
    "print(\"Using random seed: %d\" % seed_value)\n",
    "environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value) # tensorflow 2.x\n",
    "\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './imagenette2-320'\n",
    "TRAIN_DIR = path.join(DATA_DIR, 'train')\n",
    "VALIDATION_DIR = path.join(DATA_DIR, 'validation') # a split off 'train' used as validation set during NN training\n",
    "TEST_DIR = path.join(DATA_DIR, 'val') # the original Imagenette test dir\n",
    "MODELS_DIR = path.join('./models')\n",
    "LABELS_FILE = path.join(DATA_DIR, 'noisy_imagenette.csv')\n",
    "CLASS1 = 'n03445777' # e.g. n03445777 -> golf ball\n",
    "CLASS2 = 'n03888257' # e.g. n03888257 -> parachute\n",
    "CLASSES = [CLASS1, CLASS2]\n",
    "IMG_SIZE = (150, 150)\n",
    "COLOUR_SCALE = 1/255.\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation flow from train/validation directory\n",
    "def create_flow(datagen, path, batch_size):\n",
    "    return datagen.flow_from_directory(\n",
    "        path,\n",
    "        target_size=IMG_SIZE,\n",
    "        classes=CLASSES,\n",
    "        class_mode='binary',\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "def evaluate_model(model_file):\n",
    "    model = models.load_model(model_file)\n",
    "    test_flow = create_flow(ImageDataGenerator(rescale=COLOUR_SCALE), TEST_DIR, BATCH_SIZE)\n",
    "    loss_accuracy = model.evaluate(test_flow, steps=test_flow.samples // BATCH_SIZE, verbose=False)\n",
    "    print('Test loss: %.2f, accuracy: %.2f%%' % (loss_accuracy[0], loss_accuracy[1] * 100.))\n",
    "    \n",
    "def plot_model_history(history):\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    \n",
    "    # Binary Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['binary_accuracy'])\n",
    "    plt.plot(history.history['val_binary_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('binary_accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def load_random_image(filepath):\n",
    "    img_file = random.choice(os.listdir(filepath))\n",
    "    img = imgproc.load_img(path.join(filepath, img_file))\n",
    "    img = img.resize(IMG_SIZE)\n",
    "    img_array = imgproc.img_to_array(img)\n",
    "    return img_array * COLOUR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make train/validation split and organize directory structure accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.isdir(VALIDATION_DIR):\n",
    "    ground_truth = pd.read_csv(LABELS_FILE)\n",
    "    ground_truth = ground_truth[ground_truth['noisy_labels_0'].isin(CLASSES)]\n",
    "    test_df = ground_truth[ground_truth['is_valid']==True]\n",
    "    imagenette_train = ground_truth[ground_truth['is_valid']==False]\n",
    "    train_df, val_df = train_test_split(imagenette_train, test_size=0.2) # the dataset is balanced\n",
    "    val_df = val_df.rename(columns={'path': 'orig_path'})\n",
    "    val_df['path'] = val_df['orig_path'].str.replace('train/', 'validation/')\n",
    "    val_df.apply(lambda v: os.renames(path.join(DATA_DIR, v['orig_path']), path.join(DATA_DIR, v['path'])), axis=1)\n",
    "    del val_df['orig_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train baseline CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ImageDataGenerator(rescale=COLOUR_SCALE)\n",
    "baseline_model_file = path.join(MODELS_DIR, 'imagenette', 'baseline.h5')\n",
    "if not path.isfile(baseline_model_file):\n",
    "    os.makedirs(path.dirname(baseline_model_file), exist_ok=True)\n",
    "    \n",
    "    # Define model architecture\n",
    "    inputs = layers.Input(shape=IMG_SIZE + (3,))\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    baseline_model = keras.Model(inputs=inputs, outputs=output)\n",
    "    baseline_model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    epochs_count = 35\n",
    "    datagen = ImageDataGenerator(rescale=COLOUR_SCALE)\n",
    "    train_flow = create_flow(datagen, TRAIN_DIR, BATCH_SIZE)\n",
    "    val_flow = create_flow(datagen, VALIDATION_DIR, BATCH_SIZE)\n",
    "    save_best_cb = callbacks.ModelCheckpoint(filepath=baseline_model_file,\n",
    "                                             monitor='val_loss', mode='min', save_best_only=True,\n",
    "                                             verbose=False) # set to True to see best model's epoch\n",
    "    baseline_model.compile(\n",
    "        optimizers.RMSprop(lr=1e-4),\n",
    "        'binary_crossentropy',\n",
    "        metrics=[metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    history = baseline_model.fit(train_flow, steps_per_epoch=train_flow.samples // BATCH_SIZE,\n",
    "                       validation_data=val_flow, validation_steps=val_flow.samples // BATCH_SIZE,\n",
    "                       epochs=epochs_count,\n",
    "                       callbacks=[save_best_cb],\n",
    "                       verbose=False)\n",
    "    plot_model_history(history)\n",
    "\n",
    "# Evaluate against test dataset\n",
    "evaluate_model(baseline_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use data augmentation to improve model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "improved_model_file = path.join(MODELS_DIR, 'imagenette', 'improved.h5')\n",
    "if not path.isfile(improved_model_file):\n",
    "    os.makedirs(path.dirname(improved_model_file), exist_ok=True)\n",
    "    \n",
    "    # Define model architecture\n",
    "    inputs = layers.Input(shape=IMG_SIZE + (3,))\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    improved_model = keras.Model(inputs=inputs, outputs=output)\n",
    "    improved_model.summary()\n",
    "\n",
    "    # Train model\n",
    "    epochs_count = 100\n",
    "    train_gen = ImageDataGenerator( # TODO document choices\n",
    "        rescale=COLOUR_SCALE,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    train_flow = create_flow(train_gen, TRAIN_DIR, BATCH_SIZE)\n",
    "    val_flow = create_flow(ImageDataGenerator(rescale=COLOUR_SCALE), VALIDATION_DIR, BATCH_SIZE)\n",
    "    save_best_cb = callbacks.ModelCheckpoint(filepath=improved_model_file,\n",
    "                                             monitor='val_loss', mode='min', save_best_only=True,\n",
    "                                             verbose=False) # set to True to see best model's epoch\n",
    "    improved_model.compile(\n",
    "        optimizers.RMSprop(lr=1e-4),\n",
    "        'binary_crossentropy',\n",
    "        metrics=[metrics.BinaryAccuracy()]\n",
    "    )\n",
    "    history = improved_model.fit(train_flow, steps_per_epoch=val_flow.samples // BATCH_SIZE,\n",
    "                       validation_data=val_flow, validation_steps=val_flow.samples // BATCH_SIZE,\n",
    "                       epochs=epochs_count,\n",
    "                       callbacks=[save_best_cb],\n",
    "                       verbose=False)\n",
    "    plot_model_history(history)\n",
    "\n",
    "# Evaluate against test dataset\n",
    "evaluate_model(improved_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize intermediate activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = models.load_model(path.join(MODELS_DIR, 'imagenette', 'improved.h5'))\n",
    "\n",
    "layer_outputs = []\n",
    "for each in model.layers:\n",
    "    if 'conv2d' in each.name:\n",
    "        layer_outputs.append(each.output)\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 255)) # makes activation values ready for drawing\n",
    "channels_per_row = 32\n",
    "for c in CLASSES:\n",
    "    img = load_random_image(path.join(TRAIN_DIR, c))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    img = np.expand_dims(img, axis=0) # array with a single image\n",
    "    activations = activation_model.predict(img) \n",
    "    for layer_activation in activations:\n",
    "        layer_channel_count = layer_activation.shape[-1]\n",
    "        img_size = layer_activation.shape[1]\n",
    "        row_count = layer_channel_count // channels_per_row\n",
    "        display_grid = np.zeros((img_size * row_count, channels_per_row * img_size))\n",
    "        for row in range(row_count):\n",
    "            for col in range(channels_per_row):\n",
    "                channel_image = layer_activation[0, # CNN layer has a single output tensor\n",
    "                                                 :, :, # the activations for each output channel\n",
    "                                                 row * channels_per_row + col] # row and offset in the display grid\n",
    "                # scale the activations and populate in display grid\n",
    "                channel_image = scaler.fit_transform(channel_image)\n",
    "                display_grid[row * img_size : (row + 1) * img_size, col * img_size : (col + 1) * img_size] = channel_image\n",
    "        \n",
    "        # draw\n",
    "        plt.figure(figsize=(display_grid.shape[1] / img_size, display_grid.shape[0] / img_size))\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='pink')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
